<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://girishg0110.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://girishg0110.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-10T00:37:43+00:00</updated><id>https://girishg0110.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html"></title><link href="https://girishg0110.github.io/blog/2025/2024-12-18-basel-problem/" rel="alternate" type="text/html" title=""/><published>2025-06-10T00:37:43+00:00</published><updated>2025-06-10T00:37:43+00:00</updated><id>https://girishg0110.github.io/blog/2025/2024-12-18-basel-problem</id><content type="html" xml:base="https://girishg0110.github.io/blog/2025/2024-12-18-basel-problem/"><![CDATA[<h2 id="a-brief-history">A Brief History</h2> <p>In 1650, Pietro Mengoli published a paper titled “Novae quadraturae arithmeticae seu de additione fractionum” (loosely translated as “New arithmetic calculations of the sums of fractions”) in which he examined several series that intrigued his mathematical contemporaries. Among his results were that the harmonic series, defined as the sum of the reciprocals of the natural numbers, was divergent; that the alternating harmonic series converged to $\ln{2}$; and that the Wallis product converged to $\pi$. However, one problem that stumped even him was to calculate the value of the following series:</p> <p>$\begin{align}\sum_{n=1}^\infty \frac{1}{n^2}\end{align}$</p> <p>For the next century, an increasing number of mathematical pioneers grew increasingly frustrated at the seeming intractability of this very natural generalization of the harmonic series. They could prove, as Jakob Bernoulli did, that this series converged by comparison to other well-known series, but the exact value was unknown. Worse still, the series was found to converge at a tedious pace, so guessing the closed form solution from approximations was out of the question.</p> <p>Just when it seemed that no mathematician (and no mathematical tools) were up to the task of calculating the value of this series exactly, Leonhard Euler published a most unusual result in 1735: $\begin{align}\sum_{n=1}^\infty \frac{1}{n^2}=1+\frac{1}{2^2}+\frac{1}{3^2}+\frac{1}{4^2}…=\frac{\pi^2}{6}\end{align}$</p> <p>While his methods are considered dubious by modern standards, his end result appeared sound: more efficient approximations of the Basel series that Euler himself introduced showed that the true value was exceedingly closed to refined experimental evidence. Indeed, he was right, and his manipulations of infinite polynomials as if they were finite were later shown to be mathematically valid.</p> <p>Since then, mathematicians have both polished up Euler’s own original proof and constructed ingenious solutions of their own to the same problem, drawing on disparate fields of study like analysis, geometry, and probability. I will show three such proofs in this paper, all leading to the same remarkable result.</p> <h2 id="proof-1-using-the-taylor-series-for-arcsinx">Proof 1: Using the Taylor Series for arcsin(x)</h2> <p>In this section, we will solve exercises from section 8.3 of Abbott’s Understanding Analysis following a proof by Boo Rim Choe refined by Peter Duren.</p> <h3 id="exercise-833">Exercise 8.3.3</h3> <p>Our motivation in this exercise is to derive a closed-form solution for one class of the so-called Wallis integrals, a formula that will prove helpful in the final stages of this proof.</p> <p>Let $b_n=\int_{x=0}^{\pi/2} \sin^{n}(x) dx$ and assume $n\geq 2$. Note that $\sin^{n}(x)=\sin^{n-1}(x)\cdot \sin(x)$, so we can do integration by parts with $u=\sin^{n-1}(x)$ and $dv=\sin(x)dx$.</p> <p>$\begin{align}b_{n}=[-\sin^{n-1}(x)\cos(x)]^{\pi/2}_0+\int_0^{\pi/2}(n-1)\sin^{n-2}(x)\cos^2(x)dx\end{align}$</p> <p>At $x=0$, $\sin(x)=0$. At $x=\pi/2$, $\cos(x)=0$. Then, this first term is 0. As for the second integral, we can rewrite $\cos^2(x)$ as $1-\sin^2{x}$ according to familiar Pythagorean identities.</p> <p>$\begin{align}=(n-1)\int_0^{\pi/2} \sin^{n-2}(x)(1-\sin^2(x)) dx\end{align}$ $\begin{align}=(n-1)[\int_0^{\pi/2} \sin^{n-2}(x) - \sin^{n-2}(x)\sin^2(x) dx]\end{align}$ $\begin{align}=(n-1)[\int_0^{\pi/2} \sin^{n-2}(x) dx - \int_0^{\pi/2} \sin^{n}(x) dx]\end{align}$</p> <p>Observe that these two integrals are also Wallis integrals! Then, some algebra follows and we arrive at the following recurrence relation:</p> <p>$\begin{align}b_n=(n-1)[b_{n}+b_{n-2}] \implies b_n=\frac{n-1}{n}(b_{n-2})\end{align}$</p> <p>Wallis integrals behave differently for even and odd $n$. For our proof, we need to investigate Wallis integrals of odd $n$. Let $n=2k+1$ for non-negative integers $k$. The base case is easily solved, as $b_1=\int_0^{\pi/2}\sin(x) dx=[\cos(x)]<em>0^{\pi/2}=1$. The explicit formula for $b</em>{2k+1}$ is then</p> <p>$\begin{align}\Pi_{i=0}^k \frac{2i}{2i+1}=\frac{2^k\cdot k!}{\Pi_{i=0}^k {2i+1}} = \frac{2^k\cdot k!}{1\cdot 3\cdot 5…\cdot (2k+1)}\end{align}$ $\begin{align}=\frac{2^k\cdot k!}{\frac{(2k+1)!}{2\cdot 4\cdot 6… \cdot (2k)}}=\frac{2^k\cdot k!}{\frac{(2k+1)!}{2^k \cdot k!}}=\frac{2^{2k}(k!)^2}{(2k+1)!}\end{align}$</p> <h3 id="exercise-836">Exercise 8.3.6</h3> <p>Recall that this proof will arrive at the solution to the Basel problem through a Taylor representation of the $\arcsin{x}$ function. While the $\arcsin{x}$ function itself maybe be a bit unwieldy, it has a very simple derivative: $\frac{1}{\sqrt{1-x^2}}$. Then, the idea is to find a Taylor series for an even simpler expression $\frac{1}{\sqrt{1-x}}$, to substitute $x\to x^2$, and finally to term-wise integrate (with some justification) to find a Taylor series for $\arcsin{x}$ itself.</p> <p>To find the Taylor series expansion of $(1-x)^{-\frac{1}{2}}$, we can use the Taylor coefficient formula: $c_n=\frac{f^{(n)}(0)}{n!}$.</p> <p>$\begin{align}c_0=1\end{align}$ $\begin{align}c_1=\frac{f^{(1)}(0)}{1!}=\frac{\frac{1}{2}(1-0)^{-\frac{3}{2}}}{1!}=\frac{1}{2}\end{align}$ $\begin{align}c_2=\frac{f^{(2)}(0)}{2!}=\frac{\frac{1}{2}\cdot \frac{3}{2}(1-0)^{-\frac{5}{2}}}{2!}=\frac{1\cdot 3}{2^2\cdot 2!}\end{align}$ $\begin{align}c_3=\frac{f^{(3)}(0)}{3!}=\frac{\frac{1}{2}\cdot \frac{3}{2}\frac{5}{2}(1-0)^{-\frac{7}{2}}}{3!}=\frac{1\cdot 3\cdot 5}{2^3\cdot 3!}\end{align}$ $\begin{align}c_n=\frac{f^{(n)}(0)}{n!}=\frac{\frac{1}{2}\cdot \frac{3}{2}\frac{5}{2}…\frac{2n-1}{2}}{n!}=\frac{1\cdot 3\cdot 5\cdot … \cdot 2n-1}{2^n\cdot n!}\end{align}$</p> <p>Rewriting the denominator yields $c_n=\frac{\Pi_{i=1}^n 2i-1}{\Pi_{i=1}^n 2i}$ for $n&gt;0$. Using the standard techniques mentioned earlier in this section for rewriting products of this form, $c_n=\frac{(2n)!}{2^{2n}(n!)^2}$ for $n&gt;0$.</p> <h3 id="exercise-837a">Exercise 8.3.7a</h3> <p>In this exercise, our interest in is proving that $\lim_{n\to \infty} c_n$ as defined before will be equal to 0. This will be helpful when proving that the error function of a certain Taylor expansion approaches 0. A useful result is provided to us in Abbott and stated here without proof:</p> <p>$\begin{align}\lim_{n\to \infty} \frac{1}{c_n\sqrt{n}} = \sqrt{\pi}\end{align}$</p> <p>Suppose $\lim c_n\neq 0$. Then, by the Algebraic Limit Theorem,</p> <p>$\begin{align}\lim_{n\to \infty} \frac{1}{c_n\sqrt{n}} = \lim_{n\to \infty} \frac{\frac{1}{\sqrt{n}}}{c_n} = \frac{ \lim_{n\to \infty} \frac{1}{\sqrt{n}} }{ \lim_{n\to \infty} c_n }\end{align}$</p> <p>The limit in the numerator is clearly 0, and the limit in the denominator is non-zero. Then, this quotient (and the original limit we were examining) is equal to 0. But we know that $\lim_{n\to \infty} \frac{1}{c_n\sqrt{n}} = \sqrt{\pi}\neq 0$. This is a contradiction. Then, $\lim_{n\to \infty} c_n = 0$.</p> <h3 id="exercise-839ab">Exercise 8.3.9ab</h3> <p>Now, we need to prove that this Taylor series expansion for $\frac{1}{\sqrt{1-x}}$ converges for $|x|&lt;1$. However, we will need to introduce a new tool to estimate the error function $E_N(x)$, something better than the Lagrange error bound. We prove the bound suggested by the Integral Remainder Theorem:</p> <p>$\begin{align}E_N(x)=\frac{1}{N!}\int_0^x f^{N+1}(t)(x-t)^N dt\end{align}$</p> <ul> <li>We can rewrite the integral $\int_{0}^x f’(t) dt$ as $f(x)-f(0)$ by the Fundamental Theorem of Calculus. Rearranging yields $f(x)=f(0)+\int_{0}^x f’(t)$. This is the first iteration of the Integral Remainder Theorem.</li> <li>Next, we apply integration by parts to the integral $\int_{0}^x f’(x) dt$ in the previous problem to obtain a second iteration, where the error is a function of the second derivative. $\begin{align}\int_{0}^x f’(t) dt=f’(x)(x-x)+f’(0)(x-0)+\int_{0}^x f”(t)(x-t) dt\end{align}$ $\begin{align}=xf’(0)+\int_{0}^x f”(t)(x-t) dt\end{align}$ Substituting this expression into the expression from part A of this problem yields the following, as desired: $\begin{align}f(x)=f(0)+f’(0)x+\int_{0}^x f”(t)(x-t) dt\end{align}$</li> <li>A general pattern is observed about the results of this iterative integration-by-parts. Based on this observation, we consider the general case of integrating $\int_0^x f^{(n)}(t) \frac{(x-t)^{n-1}}{(n-1)!}dt$ by parts. $\begin{align}\int_0^x f^{(n)}(t) \frac{(x-t)^{n-1}}{(n-1)!}dt=f^{(n)}(t)\frac{(x-t)^{n}}{n}\frac{1}{(n-1)!}+\int_0^x f^{(n+1)}(t) \frac{(x-t)^n}{n!} dt\end{align}$ $\begin{align}=f^{(n)}(t)\frac{(x-t)^{n}}{n!}+\int_0^x f^{(n+1)}(t) \frac{(x-t)^n}{n!} dt\end{align}$ As expected, a integral bound involving $f^{(n)}$ is broken into a term outside an integral and an integral bound involving $f^{(n+1)}$, in the form expected by the Integral Remainder Theorem. This process can be repeated how many ever times $f$ is differentiable (which agrees with what is stated in Abbott).</li> </ul> <h3 id="exercise-8310d">Exercise 8.3.10d</h3> <p>With this new tool for estimating the error of a Taylor polynomial, we show that the Taylor series for $\frac{1}{\sqrt{1-x}}$ converges on $(-1,1)$.<br/> First, we use the specific case of $f(x)=\frac{1}{\sqrt{1-x}}$ to derive an explicit formulation of the Integral Remainder Theorem error $E_N=\frac{1}{N!}\int_0^x f^{(N+1)}(t)(x-t)^N dt$. In particular, we can rewrite $f^{(N+1)}(t)$.</p> <p>$\begin{align}f^1(t)=\frac{1}{2}(1-t)^{-\frac{1}{2}-1}\end{align}$ $\begin{align}f^2(t)=\frac{1}{2}\frac{3}{2}(1-t)^{-\frac{1}{2}-2}\end{align}$ $\begin{align}f^{N+1}(t)=(\frac{1}{2}\frac{3}{2}…\frac{2(N+1)-1}{2})(1-t)^{-\frac{1}{2}-N-1}\end{align}$ $\begin{align}=(1-t)^{-\frac{3}{2}}(1-t)^{-N}\Pi_{i=0}^N \frac{2i+1}{2}\end{align}$ $\begin{align}=(1-t)^{-\frac{3}{2}}(1-t)^{-N}\frac{1\cdot 3\cdot … \cdot (2N+1)}{2^{N+1}}\end{align}$ $\begin{align}=(1-t)^{-\frac{3}{2}}(1-t)^{-N}\frac{\frac{(2N+1)!}{2^{N}N!}}{2^{N+1}} =(1-t)^{-\frac{3}{2}}(1-t)^{-N}\frac{(2N+1)!}{2^{2N+1}N!}\end{align}$ Now, we can substitute this expression into the formula for $E_N$. $\begin{align}E_N=\frac{1}{N!}\int_0^x (1-t)^{-\frac{3}{2}}(1-t)^{-N}\frac{(2N+1)!}{2^{2N+1}N!}(x-t)^N dt\end{align}$ Taking constant terms out of the integral and rearranging terms gives us a clearer sense of what is happening. $\begin{align}E_N=\frac{(2N+1)!}{2^{2N+1}(N!)^2}\int_0^x (1-t)^{-\frac{3}{2}}(\frac{x-t}{1-t})^N dt\end{align}$ If we could reduce the dependence of the integrand on $t$, i.e. by bounding the expression $\frac{x-t}{1-t}$ with a function of $x$, the integral is trivial to compute. Recall that we are working under the assumption that $|x|&lt;1$. Let us first consider the case of $0\leq x\leq 1$. Then, $0\leq t\leq x$. Then, $0\leq xt\leq t$. Focusing on the second part of this inequality, we can multiply both sides by $-1$ and add $x$: $x-xt\geq x-t$. Both expressions are positive, so we can take the absolute value of both sides and divide through to obtain $|x|\geq |\frac{x-t}{1-t}|$. The case of $-1\leq x\leq 0$ is analogous. <br/> Then, we can upper bound $E_N$ by</p> <table> <tbody> <tr> <td>$\begin{align}E_N\leq \frac{(2N+1)!}{2^{2N+1}(N!)^2}</td> <td>x</td> <td>^N\int_0^x (1-t)^{-\frac{3}{2}} dt\end{align}$</td> </tr> </tbody> </table> <p>The integral can now be manually computed as $\int_0^x (1-t)^{-\frac{3}{2}} dt=2(1-x)^{-\frac{1}{2}}-2$. Then,</p> <table> <tbody> <tr> <td>$\begin{align}E_N\leq \frac{(2N+1)!}{2^{2N}(N!)^2}</td> <td>x</td> <td>^N(\frac{1}{\sqrt{1-x}}-1)=(2N+1)\cdot c_{N}\cdot</td> <td>x</td> <td>^N\cdot (\frac{1}{\sqrt{1-x}}-1)\end{align}$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>For any chosen $x\in(-1,1)$, $\frac{1}{\sqrt{1-x}}-1$ is a constant factor. We know that $\lim_{n\to \infty} c_n = 0$ from our work on Exercise 8.3.7a. All that remains to show is that $\lim_{N\to \infty} (2N+1)</td> <td>x</td> <td>^N = 0$, and this is evident because the $</td> <td>x</td> <td>^N$ decays exponentially towards 0 while the $2N+1$ term only grows linearly. Then, $\lim_{N\to \infty} E_N = 0$, which implies that our Taylor series representation for $\frac{1}{\sqrt{1-x}}$ converges uniformly for all $</td> <td>x</td> <td>&lt;1$.</td> </tr> </tbody> </table> <h3 id="exercise-8311">Exercise 8.3.11</h3> <p>Now, we can substitute $x\to x^2$ into our Taylor series for $\frac{1}{\sqrt{1-x}}$ to obtain the following series, defined for $|x|&lt;1$:</p> <p>$\begin{align}\frac{1}{\sqrt{1-x^2}} = \sum_{n=0}^\infty c_n x^{2n}\end{align}$</p> <p>Everything is in place for us to find the Taylor series for $\arcsin{x}$. We know that $\sum_{n=0}^\infty c_n x^{2n}$ converges uniformly for $x\in(-1,1)$ and that $\arcsin{x} = \int_0^x \frac{1}{\sqrt{1-x^2}} dx$. Then,</p> <p>$\begin{align}\arcsin{x}=\int_0^x \sum_{n=0}^\infty c_n x^{2n} dx\end{align}$</p> <p>Since the inner series is uniformly convergent, we can bring the integral into the sum like so by the Integrable Limit Theorem:</p> <p>$\begin{align}=\sum_{n=0}^\infty \int_0^x c_n x^{2n} dx\end{align}$</p> <p>Evaluating the integral yields</p> <p>$\begin{align}=\sum_{n=0}^\infty \frac{c_n}{2n+1}x^{2n+1}\end{align}$</p> <table> <tbody> <tr> <td>as a series expansion for $\arcsin{x}$ for $</td> <td>x</td> <td>&lt;1$.</td> </tr> </tbody> </table> <h3 id="exercise-8312">Exercise 8.3.12</h3> <p>Now, we can extend the domain of the Taylor series for $\arcsin{x}$ from $(-1,1)$ to $[-1,1]$. First, consider the case of $x=1$. Substituting $x=1$ into the expansion yields</p> <p>$\begin{align}\sum_{n=0}^\infty \frac{c_n}{2n+1}=1 + \sum_{n=1}^\infty \frac{1}{(2n+1)\sqrt{n}}c_n\sqrt{n}\end{align}$</p> <p>Recall that $\lim_{n\to \infty} \frac{1}{c_n\sqrt{n}} = \sqrt{\pi}$. Then, $\lim_{n\to \infty} c_n\sqrt{n} = \frac{1}{\sqrt{\pi}}$. Since $c_n\sqrt{n}$ is increasing, we can bound $c_n\sqrt{n}\leq \frac{1}{\pi}$. Then, $0\leq \sum_{n=1}^\infty \frac{c_n}{2n+1}\leq \frac{1}{\sqrt{\pi}}\sum_{n=1}^\infty \frac{1}{(2n+1)\sqrt{n}}$. But $\sum_{n=1}^\infty \frac{1}{(2n+1)\sqrt{n}}\leq \sum_{n=1}^\infty \frac{1}{2n^{3/2}}$, which converges by the p-series test. Then, this series expansion converges point-wise at $x=1$.</p> <p>To extend this result to $x=-1$, we start by substituting $x=-1$ into the $\arcsin{x}$ expansion as before.</p> <p>$\begin{align}=\sum_{n=0}^\infty \frac{c_n}{2n+1}(-1)^{n}\end{align}$</p> <table> <tbody> <tr> <td>We can use the Absolute Convergence Test here. If $\sum_{i=1}^n</td> <td>\frac{c_n}{2n+1}(-1)^{n}</td> <td>=\sum_{i=1}^n \frac{c_n}{2n+1}$ converges, so does the original series. But this is the series we examined when trying to prove that $\arcsin{x}$ converges at $x=1$. Then, the expansion for $\arcsin{x}$ converges point-wise at $x=-1$ as well.</td> </tr> </tbody> </table> <h3 id="exercise-8313">Exercise 8.3.13</h3> <p>All our groundwork will now come to fruition. We note that since $\arcsin{x}$ is the inverse function of $\sin{x}$, $\arcsin(\sin{x})=x$ for $x\in (-\frac{\pi}{2},\frac{\pi}{2})$.</p> <ul> <li>We make the substitution $x=\sin{\theta}$ in the $\arcsin{x}$ sum. Then, we know that since the sum converges uniformly from Exercise 8.3.12, we can integrate this series term-wise as such: $\begin{align}\int_0^{\pi/2}\theta d\theta=\int_0^{\pi/2}\sum_{n=0}^\infty \frac{c_n}{2n+1}\sin^{2n+1}(\theta) d\theta\end{align}$ First, we bring the integral inside the sum. This is allowed by the Integrable Limit Theorem as the sum converges uniformly on the interval $[-\pi/2,\pi/2]$. $\begin{align}=\sum_{n=0}^\infty \int_0^{\pi/2} \frac{c_n}{2n+1}\sin^{2n+1}(\theta) d\theta\end{align}$ We bring the terms that are not dependent on $\theta$ outside the integral, as they are constant factors. $\begin{align}=\sum_{n=0}^\infty \frac{c_n}{2n+1}\int_0^{\pi/2}\sin^{2n+1}(\theta) d\theta\end{align}$ Here, the form of the odd Wallis integral is recognized. We make the substitution $b_{2n+1}=\int_0^{\pi/2}\sin^{2n+1}(\theta) d\theta$. $\begin{align}=\sum_{n=0}^\infty \frac{c_n}{2n+1}b_{2n+1}\end{align}$</li> <li> <p>The integral on the left-hand side is simple to calculate directly. $\begin{align}\int_0^{\pi/2} \theta d\theta=\frac{(\pi/2)^2}{2}=\pi/8\end{align}$ To evaluate the integral on the right-hand side, we need to simplify the product $c_n\cdot b_{2n+1}$. We know that $c_n=\frac{(2n)!}{2^{2n}(n!)^2}$ and $b_{2n+1}=\frac{2^{2n}(n!)^2}{(2n+1)!}$ from our earlier work. Then,</p> <p>$\begin{align}c_n\cdot b_{2n+1} = \frac{(2n)!}{2^{2n}(n!)^2} \cdot \frac{2^{2n}(n!)^2}{(2n+1)!}=\frac{1}{2n+1}\end{align}$</p> <p>Then, $\frac{\pi}{8}=\sum_{i=0}^\infty \frac{1}{(2n+1)}\cdot \frac{1}{2n+1}$. We’re in the home stretch now. Note that the sum we desire can be broken into two component sums representing the reciprocals of all even and all odd squares, respectively:</p> <p>$\begin{align}\sum_{n=1}^\infty \frac{1}{n^2}=\sum_{n=1}^\infty \frac{1}{(2n)^2} + \sum_{n=1}^\infty \frac{1}{(2n-1)^2}\end{align}$</p> <p>This latter sum is what we have calculated to be $\frac{\pi}{8}$. The former sum is seen by inspection to be one-fourth of the desired sum: $\sum_{n=1}^\infty \frac{1}{(2n)^2} = \frac{1}{2^2}\sum_{n=1}^\infty \frac{1}{n^2}$. Then, letting $A=\sum_{n=1}^\infty \frac{1}{n^2}$, we have a simple linear equation in $A$, which leads us to the solution of the Basel problem:</p> <p>$\begin{align}A=\frac{1}{4}A + \frac{\pi}{8}\implies \boxed{ \sum_{n=1}^\infty \frac{1}{n^2}=\frac{\pi}{6} }\end{align}$</p> </li> </ul> <h2 id="proof-2-by-trigonometry-and-eulers-formula">Proof 2: By Trigonometry and Euler’s Formula</h2> <p>This next proof is taken from Cambridge University’s Sixth Term Examination Paper from 2018. Its structure goes as follows: (1) we establish the validity of a certain trigonometric identity, (2) we link this identity to a particular polynomial, and (3) we tie up all aspects to solve the Basel problem.</p> <h3 id="some-trigonometry">Some Trigonometry</h3> <p>Consider the expression $\frac{(\cot{\theta} + i)^{2n+1}-(\cot{\theta} - i)^{2n+1}}{2i}$, where $i$ is the imaginary unit. We are tasked with simplifying this expression into a form that does not depend on any imaginary units.</p> <p>This is where Euler’s Formula comes in. Euler’s Formula states that we can express real and imaginary parts of the complex exponential $e^{i\theta}$ as $\cos{\theta}$ and $i \sin{\theta}$, respectively. Note that the closest expression in our original statement to $\cos{\theta}+i\sin{\theta}$ is $\cot{\theta} + i$. After some inspection, we see that</p> <p>$\begin{align}e^{i\theta}=\cos{\theta}+i\sin{\theta}\implies \frac{e^{i\theta}}{\sin{\theta}}=\cot{\theta}+i\end{align}$</p> <p>To obtain an alternate form for $\cot{\theta} - i$, note that negating $\theta$ leaves the real part unchanged, but negates the imaginary part. In other words</p> <p>$\begin{align}e^{-i\theta}=\cos(-\theta)+i\sin(-\theta)=\cos{\theta}-i\sin{\theta}\implies \frac{e^{-i\theta}}{\sin{\theta}}=\cot{\theta}-i\end{align}$</p> <p>Then, substituting this expression back into the original yields</p> <p>$\begin{align}\frac{(\frac{e^{i\theta}}{\sin{\theta}})^{2n+1}-(\frac{e^{-i\theta}}{\sin{\theta}})^{2n+1}}{2i}\end{align}$ $\begin{align}=\frac{e^{i(2n+1)\theta}-e^{-i(2n+1)\theta}}{2i\sin^{2n+1}{\theta}}\end{align}$</p> <p>Now, we can apply Euler’s Formula once more, this time to convert the complex exponentials in the numerator into trigonometric quantities.</p> <p>$\begin{align}=\frac{\cos((2n+1)\theta)+i\sin((2n+1)\theta)-\cos(-(2n+1)\theta)-i\sin(-(2n+1)\theta)}{2i\sin^{2n+1}{\theta}}\end{align}$ $\begin{align}=\frac{2i\sin{(2n+1)\theta}}{2i\sin^{2n+1}{\theta}}=\frac{\sin{(2n+1)\theta}}{\sin^{2n+1}{\theta}}\end{align}$</p> <h3 id="introducing-a-polynomial">Introducing a Polynomial</h3> <p>Let us look at the original expression $\frac{(\cot{\theta} + i)^{2n+1}-(\cot{\theta} - i)^{2n+1}}{2i}$ through another lens. We can use the Binomial Theorem to expand both terms in the numerator.</p> <p>$\begin{align}=\frac{\sum_{k=0}^{2n+1} \binom{2n+1}{k}\cot^k{\theta}\cdot i^{2n+1-k}- \sum_{k=0}^{2n+1}\binom{2n+1}{k}\cot^k{\theta}\cdot (-i)^{2n+1-k}}{2i}\end{align}$ $\begin{align}=\frac{\sum_{k=0}^{2n+1} \binom{2n+1}{k}\cot^k{\theta}\cdot (i^{2n+1-k}-(-i)^{2n+1-k})}{2i}\end{align}$ $\begin{align}=\sum_{k=0}^{2n+1} \binom{2n+1}{2n+1-k}\cot^k{\theta}\cdot \frac{i^{2n+1-k}(1-(-1)^{2n+1-k})}{2i}\end{align}$</p> <p>For all the terms where the exponent $k$ on $\cot{\theta}$ is odd, the expression involving the imaginary unit evaluates to 0, so that term vanishes. For those terms where the exponent $k$ on $\cot{\theta}$ is even, the same expression alternates between $+2i$ and $-2i$, which cancels with the denominator. $\begin{align}\frac{(\cot{\theta} + i)^{2n+1}-(\cot{\theta} - i)^{2n+1}}{2i}=\binom{2n+1}{2n}cot^{2n}(\theta)-\binom{2n+1}{2n-2}cot^{2n-2}(\theta)…+(-1)^n\end{align}$ If we let $x=\cot^{2}{\theta}$ and rewrite binomial coefficients using familiar combinatorial rules, we obtain a polynomial equation in $x$: $\begin{align}=\binom{2n+1}{1}x^{n}-\binom{2n+1}{3}x^{n-1}+…+(-1)^n\end{align}$ At this moment in the proof, we have three equivalent interpretation of the same trigonometric identity. Now, we need to find the roots of this latest representation as a polynomial. However, since this polynomial is equivalent to the second representation we derived (i.e. as a ratio of expressions involving the $\sin(x)$ function), any roots of that expression must also be roots of this expression, and vice versa. Then, the roots of this polynomial are the roots of $\frac{\sin{(2n+1)\theta}}{\sin^{2n+1}{\theta}}$. It is clear that this expression evaluates to 0 when its numerator is 0. On the interval his happens when the argument of the sin function in the numerator is 0, or when $\theta=\frac{m\pi}{2n+1}$ for $m=1,2…n$. Then, $x=\cot^2(\frac{m\pi}{2n+1})$ for $m=1,2…n$.<br/> Next, we want to find the sum of the roots of this polynomial. By Vieta’s formulae, we know that this sum is the negative of the coefficient on the $x^{n-1}$ term divided by the coefficient on the $x^{n}$ term. Then, $\begin{align}\sum_{m=1}^n \cot^2(\frac{m\pi}{2n+1})=-\frac{-\binom{2n+1}{3}}{\binom{2n+1}{1}}=\frac{\frac{(2n+1)!}{(2n-2)!3!}}{\frac{(2n+1)!}{(2n)!1!}}=\frac{2n(2n-1)}{3!}=\frac{n(2n-1)}{3}\end{align}$</p> <h3 id="comparing-sums">Comparing Sums</h3> <p>Finally, let us make some notes about the domain we are working in. For $\theta\in(0,\pi/2)$, we are given that $0&lt;\sin{\theta}&lt;\theta&lt;\tan(\theta)$. Then, $\tan(\theta)&gt;\theta&gt;0\implies \frac{1}{\theta^2}&lt;\cot^2(\theta)$ and $0&lt;\sin{\theta}&lt;\theta\implies \frac{1}{\theta^2}&lt;\frac{1}{\sin^2\theta}=\csc^2{\theta}=1+\cot^2{\theta}$. Condensing our results into a single inequality, we have found that on this domain, $\begin{align}\cot^2{\theta} &lt; \frac{1}{\theta^2} &lt; 1 + \cot^2{\theta}\end{align}$ For any number of terms $n$, we can compare the sums of the above expressions over the roots calculated earlier in place of $\theta$. $\begin{align}\sum_{m=1}^n \cot^2(\frac{m\pi}{2n+1}) &lt; \sum_{m=1}^n (\frac{2n+1}{m\pi})^2 &lt; \sum_{m=1}^n 1 + \cot^2(\frac{m\pi}{2n+1})\end{align}$ We recognize the first and third sums as variations on the sum of roots we calculated manually earlier. Substitution of our previous result yields: $\begin{align}\frac{n(2n-1)}{3} &lt; \frac{(2n+1)^2}{\pi^2}\sum_{m=1}\frac{1}{m^2} &lt; n + \frac{n(2n-1)}{3}\end{align}$ $\begin{align}\frac{n(2n-1)}{3(2n+1)^2}\pi^2 &lt; \sum_{m=1}^n \frac{1}{m^2} &lt; \pi^2\frac{2n(n+1)}{3(2n+1)^2}\end{align}$ Now, we can take the limit of each of the bounding expressions. We find that $\lim_{n\to\infty} \frac{n(2n-1)}{3(2n+1)^2}\pi^2 = \frac{\pi^2}{2\cdot 3}=\pi^2/6$ and $\lim_{n\to\infty} \pi^2\frac{2n(n+1)}{3(2n+1)^2} = \frac{\pi^2}{2\cdot 3}=\pi^2/6$. Then, by the Squeeze Theorem, $\begin{align}\lim_{n\to \infty}\sum_{m=1}^n \frac{1}{m^2} = \boxed { \sum_{m=1}^\infty \frac{1}{m^2} = \frac{\pi^2}{6} } \end{align}$</p> <h2 id="proof-3-by-a-double-integral">Proof 3: By a Double Integral</h2> <p>This final approach is due to Tom Apostol; we will represent a double integral as a familiar infinite series, and then we will solve the double integral by making a clever substitution.</p> <h3 id="revealing-the-hidden-series">Revealing the Hidden Series</h3> <p>Consider the double integral $\int_0^1 \int_0^1 \frac{1}{1-xy} dx dy$. For points $(x,y)$ in the unit square, we know that $|xy|&lt;1$. Then, we recognize the integrand as the sum of an infinite geometric series. We can replace the integrand with the series it defines implicitly. $\begin{align}\int_0^1 \int_0^1 \frac{dx~dy}{1-xy} = \int_0^1 \int_0^1 \sum_{n=0}^\infty (xy)^n dx~dy\end{align}$ We move the integral on variable $x$ inside the sum and evaluate. $\begin{align}= \int_0^1 \sum_{n=0}^\infty y^n \int_0^1 x^n dx~dy=\int_0^1 \sum_{n=0}^\infty y^n [\frac{x^{n+1}}{n+1}]<em>0^1 dx~dy\end{align}$ $\begin{align}=\int_0^1 \sum</em>{n=0}^\infty \frac{1}{n+1} y^n dy\end{align}$ We bring the integral on variable $y$ inside the sum as before, and arrive at a familiar conclusion. $\begin{align}= \sum_{n=0}^\infty \frac{1}{n+1} \int_0^1 y^n dy = \sum_{n=0}^\infty \frac{1}{n+1} [\frac{y^{n+1}}{n+1}]<em>0^1 \end{align}$ $\begin{align}= \sum</em>{n=0}^\infty \frac{1}{(n+1)^2} = \sum_{n=1}^\infty \frac{1}{n^2}\end{align}$</p> <h3 id="a-change-of-perspective">A Change of Perspective</h3> <p>Now, let us take a second look at the original integral. In its current form, it may be difficult to evaluate. We will apply a linear transformation to take this integral from $xy$-space to $uv$-space, defined as $\begin{align}u = \frac{1}{2}(x+y), v = \frac{1}{2}(y-x)\end{align}$ To go from $uv$-space back to $xy$-space, we apply the transformations $x=u-v$ and $y=u+v$. Then, the integrand $\frac{1}{1-xy}$ in $uv$-coordinates is $\frac{1}{1-(u-v)(u+v)}=\frac{1}{1-u^2+v^2}$. When going to this new coordinate space, we need to replace the differentials $dx~dy$ with their corresponding values in $uv$-space. For this, we need to calculate the Jacobian determinant of this transformation: $\begin{align}\det(J)= \begin{vmatrix<em>} \frac{dx}{du} &amp; \frac{dx}{dv} <br/> \frac{dy}{du} &amp; \frac{dy}{dv} \end{vmatrix</em>} = \begin{vmatrix<em>} 1 &amp; -1 <br/> 1 &amp; 1 \end{vmatrix</em>} = 2\end{align}$ Then, after changing bounds into the appropriate coordinates, the region of integration is a square rotated $45^o$ clockwise with diagonal of length 1 lying along the x-axis. The integral needs to be written in two parts: $\begin{align}\int_0^{1/2} \int_{-u}^u \frac{1}{1-u^2+v^2}\cdot 2 dv du + \int_{1/2}^1 \int_{-(1-u)}^{1-u} \frac{1}{1-u^2+v^2}\cdot 2 dv du\end{align}$ The form of these two integrals is made familiar by a minor cosmetic change. The integrand $\frac{1}{1-u^2+v^2}$ can be written as $\frac{1}{1+(\frac{v}{\sqrt{1-u^2}})^2}\cdot \frac{1}{1-u^2}$. This second term is not dependent on $v$ and so can be taken outside the innermost integral; the first term is reminiscent of the derivative of the $\arctan(x)$ function, $\frac{1}{1+x^2}$. Indeed, we can make progress in both integrals by making the substitution $z=\frac{v}{\sqrt{1-u^2}}, dz = \frac{dv}{\sqrt{1-u^2}}$. From here, we solve the two component integrals by different routes entirely.\</p> <h3 id="the-first-integral-for-u-from-0-to-frac12">The First Integral, for $u$ from 0 to $\frac{1}{2}$</h3> <p>Applying this $z$-substitution yields the following: $\begin{align}\int_0^{1/2} \int_{-u}^u \frac{1}{1-u^2+v^2}\cdot 2 dv du = \int_0^{1/2} [\frac{1}{\sqrt{1-u^2}}\arctan(\frac{v}{\sqrt{1-u^2}})]_{-u}^u \cdot 2 du\end{align}$ $\begin{align}=\int_0^{1/2} \frac{1}{\sqrt{1-u^2}}[\arctan(\frac{u}{\sqrt{1-u^2}}) - \arctan(\frac{-u}{\sqrt{1-u^2}})] \cdot 2 du\end{align}$ Since $\arctan(x)$ is an odd function, we can write the same more concisely as $\begin{align}=4\int_0^{1/2} \frac{1}{\sqrt{1-u^2}}\arctan(\frac{u}{\sqrt{1-u^2}}) du\end{align}$ If we could rewrite this integral in terms of a variable $\theta$, such that $\tan(\theta)=\frac{u}{\sqrt{1-u^2}}$, the $\arctan(x)$ function would undo the $\tan(x)$ function within. This implies that $u=\sin(\theta)$. Acting on this intuition, the new integral, recast in terms of $\theta$, is $\begin{align}= 4\int_0^{\arcsin(1/2)} \frac{1}{\sqrt{1-\sin^2(\theta)}}\arctan(\frac{\sin(\theta)}{\sqrt{1-\sin^2(\theta)}}) \cos(\theta) d\theta \end{align}$ $\begin{align}= 4\int_0^{\pi/6} \frac{\cos(\theta)}{\cos(\theta)}\arctan(\tan(\theta)) d\theta\end{align}$ $\begin{align}= 4\int_0^{\pi/6} \theta d\theta = 4\cdot \frac{(\pi/6)^2}{2} = \boxed { \frac{\pi^2}{18} }\end{align}$</p> <h3 id="the-second-integral-for-u-from-frac12-to-1">The Second Integral, for $u$ from $\frac{1}{2}$ to $1$</h3> <p>The second integral takes a similar form to the first after the $z$-substitution outlined before, but with different bounds: $\begin{align}\int_{1/2}^1 \int_{-(1-u)}^{1-u} \frac{1}{1-u^2+v^2}\cdot 2 dv du \end{align}$ $\begin{align}= \int_{1/2}^1 \frac{1}{\sqrt{1-u^2}}[\arctan(\frac{1-u}{\sqrt{1-u^2}}) - \arctan(\frac{-(1-u)}{\sqrt{1-u^2}})] \cdot 2 dv du\end{align}$ $\begin{align}=4\int_{1/2}^{1} \frac{1}{\sqrt{1-u^2}}\arctan(\frac{1-u}{\sqrt{1-u^2}}) du\end{align}$ Ideally, we would able to make progress by recognizing the expression inside the $\arctan(x)$ function as the tangent of some angle. However, this form seems foreign, unlike the form encountered in the first integral. We proceed with a more general idea: to solve for $u$ in terms of $\theta$ under the assumption that $\tan(m\theta)=u$. In this case, the half-angle formula for $\tan(\theta/2)$ appears helpful: $\begin{align}\tan(\theta/2)=\frac{1-\cos(\theta)}{\sin(\theta)}\end{align}$ Then, let $u=\cos(\theta)$. This yields: $\begin{align}=4\int_{\pi/3}^{0} \frac{1}{\sqrt{1-\cos^2(\theta)}}\arctan(\frac{1-\cos(\theta)}{\sqrt{1-\cos^2(\theta)}}) (-\sin(\theta)) d\theta\end{align}$ $\begin{align}=4\int_{0}^{\pi/3} \arctan(\frac{1-\cos(\theta)}{\sin(\theta)}) d\theta\end{align}$ $\begin{align}=4\int_{0}^{\pi/3} \frac{\theta}{2} d\theta = 2\cdot \frac{(\pi/3)^2}{2} = \boxed { \frac{\pi^2}{9} }\end{align}$</p> <h3 id="comparing-evaluations">Comparing Evaluations</h3> <p>Then, we have two different representations for the same integral. One is as the solution to the Basel problem, derived by looking at the integrand as an infinite series. The other is by a change of coordinates and direct evaluation in the new space. Then, the two evaluations are equal to each other. $\begin{align}\frac{\pi^2}{18} + \frac{\pi^2}{9} = \boxed { \sum_{n=1}^\infty \frac{1}{n^2} = \frac{\pi^2}{6} }\end{align}$</p> <h2 id="an-application-in-probability">An Application in Probability</h2> <p>In this final section, we solve a practical problem in probability using the solution to the Basel problem. Consider the probability that two randomly chosen natural numbers less than $X$ are coprime to each other, i.e. that they share no common factors other than 1. We first need to classify the ways in which two numbers $A$ and $B$ can share a factor. It is clear that it suffices to show that two numbers do not have any prime common factors, in order to show that they are coprime. Then, we can calculate the probability that some prime $p$ does not divide both $A$ and $B$ as</p> <p>$\begin{align}\textup{P}(\textup{p does not divide A and B})= 1 - \textup{P}(\textup{p divides A})\cdot \textup{P}(\textup{p divides B})\end{align}$ Here, we assume that the divisors of $A$ and $B$ are distributed independently. Around one in every $p$ natural numbers in $[1,X)$ are divisible by $p$, so $\begin{align}\textup{P}(\textup{p divides A})=\textup{P}(\textup{p divides B})=\frac{1}{p}\end{align}$ $\begin{align}\textup{P}(\textup{p does not divide A and B})=1-\frac{1}{p^2}\end{align}$ We need to make sure that none of the primes up to $X$ divide both $A$ and $B$. Assuming again that the divisibility of a number by one prime says nothing about its divisibility by another (independence), we can write $\begin{align}\textup{P}(\textup{A and B have no common prime factors})=\Pi_{\textup{primes p&lt;X}}~(1-\frac{1}{p^2}) \end{align}$ Since the terms in the sequence approach $1$ and all terms are non-zero, we know that this probability is non-zero. Indeed, taking the limits as $X\to \infty$ of this product yields an infinite product over all the primes, that represents the probability that any two natural numbers (unbounded) are coprime. Denoting our desired probability $P’$, we can then write: $\begin{align}P’=\Pi_{\textup{primes p}}~(1-\frac{1}{p^2}) \implies \frac{1}{P’} = \Pi_{\textup{primes p}}~\frac{1}{1-\frac{1}{p^2}}\end{align}$ The expression inside the product is reminiscent of the infinite geometric series formula, with a ratio of $|\frac{1}{p^2}|&lt;1$. Then, we can expand each of these implicit series to arrive at $\begin{align}\frac{1}{P’}=\Pi_{\textup{primes p}}~(1 + \frac{1}{p^2} + \frac{1}{p^4} + \frac{1}{p^6} + …)\end{align}$ We can imagine expanding out this infinite product over infinite series, and consider what terms will appear. The numerator of every term will be 1. The denominator will be the product of even powers of certain primes; then, the reciprocal of every natural number that is able to be represented as the product of even powers of primes will appear in the expansion. According to the Fundamental Theorem of Arithmetic, every natural number can be represented uniquely as the product of powers of primes. Then, every reciprocal of a square number will appear in the expanded product once and exactly once. Then, $\begin{align}\frac{1}{P’}=\sum_{n=1}^\infty \frac{1}{n^2} \implies \boxed{ P’ = \frac{6}{\pi^2} }\approx 60.793\%\end{align}$ Then, there is about a 60.793\% chance that two randomly selected natural numbers are coprime.</p> <h2 id="bibliography">Bibliography</h2> <ul> <li>The Galileo Project. (1995). From http://galileo.rice.edu/Catalog/NewFiles/mengoli.html</li> <li>Pietro Mengoli - Biography. (2004). From https://mathshistory.st-andrews.ac.uk/Biographies/Mengoli/</li> <li>Abbott, S. (2016). Understanding analysis. New York: Springer.</li> <li>Cambridge STEP Exam Repository. (2018). From https://www.admissionstesting.org/for-test-takers/step/preparing-for-step/</li> <li>Apostol, T. M. (1983). A proof that euler missed: evaluating $\zeta(2)$ the easy way. The Mathematical Intelligencer, 5(3), 59–60.</li> <li>Aaron D. Abrams &amp; Matteo J. Paris (1992). The Probability That (a, b) = 1. The College Mathematics Journal, 23:1, 47.</li> </ul>]]></content><author><name></name></author></entry><entry><title type="html">Counting Tanks</title><link href="https://girishg0110.github.io/blog/2024/counting-tanks/" rel="alternate" type="text/html" title="Counting Tanks"/><published>2024-12-19T00:00:00+00:00</published><updated>2024-12-19T00:00:00+00:00</updated><id>https://girishg0110.github.io/blog/2024/counting-tanks</id><content type="html" xml:base="https://girishg0110.github.io/blog/2024/counting-tanks/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[&lt;!– Tank problem - t_i…t_n $P(N t) = P(t N)P(N)/P(t)$ $P(N t) ~\alpha~ P(t N)P(N)$ $P(t N) = \frac{1}{(N~choose~n)}$ $P(N) = $ estimates probability that N tanks were manufactured between the start of the war and the time of observation having a crazy number of tanks should be very unlikely could have a uniform distribution up to a certain maximum, then 0 probability discounted probability by r Poisson distribution - exponential?? - what should \lambda = mean #observations be - N?]]></summary></entry><entry><title type="html">Pentagon Walk</title><link href="https://girishg0110.github.io/blog/2024/pentagon-walk/" rel="alternate" type="text/html" title="Pentagon Walk"/><published>2024-12-18T00:00:00+00:00</published><updated>2024-12-18T00:00:00+00:00</updated><id>https://girishg0110.github.io/blog/2024/pentagon-walk</id><content type="html" xml:base="https://girishg0110.github.io/blog/2024/pentagon-walk/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[&lt;!–]]></summary></entry><entry><title type="html">topic modeling my group chats</title><link href="https://girishg0110.github.io/blog/2024/topic-model-gc/" rel="alternate" type="text/html" title="topic modeling my group chats"/><published>2024-12-16T00:00:00+00:00</published><updated>2024-12-16T00:00:00+00:00</updated><id>https://girishg0110.github.io/blog/2024/topic-model-gc</id><content type="html" xml:base="https://girishg0110.github.io/blog/2024/topic-model-gc/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>I downloaded the full transcript of all of my text conversations on Instagram and Facebook using the <a href="https://www.facebook.com/help/212802592074644?helpref=faq_content">Meta data download portal</a> for each platform. Here are a couple of questions I investigated using the chat data between one of my friend groups from the past year.</p> <p>Some notation will be useful for explaining the mathematical derivation of many of the techniques we will use.</p> <ul> <li>$C = {c_j}$ is the set of chat members</li> <li>$M = {m_i}$ is the set of messages.</li> <li>Each message $m_i=(t_i, c_i, d_i)$ is a tuple consisting of the timestamp $t_i$, the message sender $c_i$, and the message text (documents) $d_i$.</li> </ul> <p>In practice, the data is stored as a table, ordered by increasing timestamp, of messages tagged with their senders.</p> <h2 id="what-do-we-talk-about">What do we talk about?</h2> <p>I want to know what topics each of us talks about. Can we glean from the chat logs which of us is particularly interested in politics, sports, or tech topics? A branch of natural language processing called <strong>topic modeling</strong> describes computational methods for exactly this task.</p> <p>First, it is necessary to create some sort of numerical representation of each person’s messages. Going from a set of written texts to a number of list of numbers, while retaining the information necessary to glean meaningful language insights, is a non-trivial task. Bag-of-word embeddings are one possible approach to quantifying linguistic data.</p> <h2 id="embedding-bag-of-words-vectors">Embedding: bag-of-words vectors</h2> <p>To create a BoW embedding for a given sender $c$, we perform the following algorithm.</p> <ol> <li>Determine the vocabulary $V$ of unique meaningful “tokens” from $D = {d_i}$. A token can be as simple as a word separated by whitespace. You may also want to ensure that compound words and full names, like “electoral college” or “Lebron James”, are considered as single tokens. It’s also advisable to filter out punctuation marks, unreadable characters, and numbers, unless the high incidence of those characters would indicate something meaningful to you.</li> <li> <table> <tbody> <tr> <td>Determine the set of documents $D_c={d\in D~</td> <td>~sender(d) = c}$ sent by $c$.</td> </tr> </tbody> </table> </li> <li>For each vocabulary token $v\in V$, let $freq_{v,c}$ denote how many times $v$ occurs in the texts of $D_c$. Then,</li> </ol> <p>$\begin{align} bow_c = \begin{pmatrix}freq_{v_1,c}\ freq_{v_2,c}\ …\ freq_{v_{|V|},c}\end{pmatrix} \in \mathbb{Z_{\geq 0}}^{|V|} \end{align}$</p> <table> <tbody> <tr> <td>The BoW embedding $bow_c$ will be a $</td> <td>V</td> <td>$-dimensional vector such that the $i$-th coordinate of $bow_c$ equals $freq_{v_i,c}$.</td> </tr> </tbody> </table> <h2 id="reweighting-tf-idf">Reweighting: tf-idf</h2> <h2 id="whats-in-the-news">What’s in the news?</h2> <h2 id="whos-active-when">Who’s active when?</h2> <h2 id="whats-the-mood">What’s the mood?</h2> <h2 id="who-has-the-last-word">Who has the last word?</h2> ]]></content><author><name>Girish Ganesan</name></author><category term="topic-modeling"/><category term="data-viz"/><category term="social-science"/><summary type="html"><![CDATA[data visualizations and nlp insights from e-conversations]]></summary></entry><entry><title type="html">Quantum Max Cut</title><link href="https://girishg0110.github.io/blog/2024/quantum-max-cut/" rel="alternate" type="text/html" title="Quantum Max Cut"/><published>2024-12-15T00:00:00+00:00</published><updated>2024-12-15T00:00:00+00:00</updated><id>https://girishg0110.github.io/blog/2024/quantum-max-cut</id><content type="html" xml:base="https://girishg0110.github.io/blog/2024/quantum-max-cut/"><![CDATA[<p>\section*{Abstract} Quantum approximate optimization algorithms (QAOA) are a class of quantum algorithms applicable to combinatorial optimization problems. In this paper, I present a general overview of the process of adapting a problem to a QAOA-based approach. Then, I apply QAOA to the Max-Cut Problem for unweighted graphs, showing the process of developing the appropriate circuit in Qiskit and assessing the validity of my measurements. }</p> <p>\section{Overview} Currently, the quantum computers in development are extremely limited in their capabilities. A number of engineering challenges have limited progress in building quantum computers suitable for running the most important and promising applications physicists have envisioned. The implementation of Shor’s algorithm for factorization of large primes and efficient quantum chemistry simulations both rely on large numbers of qubits, each of which needs to be error-checked and kept in the strictest isolation possible for the duration of the computation. \ <br/> In contrast, QAOA is an application that is low-cost in the number of qubits, can achieve state-of-the-art performance on par or exceeding that of classical computers, and is achieved through interfacing with classical computation, removing the need for excessive fault tolerance. We set up the combinatorial optimization scenario as follows. A discrete variable $x$ can assume values in the domain $S$ and is equipped with a cost function $C(x):S\to \mathbb{R}$ that we wish to maximize. The QAOA approach is to develop an “ansatz”, or a parametrized circuit architecture, informed by the nature of the problem. We then shift focus to find the optimal parameters such that when the state $\ket{x}$ is prepared as input, the circuit returns the state $\ket{C(x)}$ as measured output. This optimization process is iterative, with each run of the algorithm getting closer to the true result.</p> <p>\subsection*{Literature Review} QAOA was originally propsed in the paper “A Quantum Approximate Optimization Algorithm” by Farhi et al (2014)$^1$. To bolster the case in favor of using QAOA as a practical, robust approach to solve combinatorial optimization problems, the authors present two notable arguments. The first lies in their investigation of $k$-regular graphs. A $k$-regular graph is one in which every vertex has exactly $k$ neighbors. By classifying all types of 3-regular graphs, they present a convincing argument that the approximation ratio of a QAOA Max-Cut circuit has a worst-case value of 0.6924. The approximation ratio for some possible partition of the graph (in the Max-Cut setting) is the value of the cut indicated by the partition divided by the optimal value of the objective, i.e. the maximal cut possible. Worst-case ratios are also provided for special kinds of $n$-regular graphs for $n&gt;3$ from a rigorous mathematical standpoint, adding credibility to the capacity of QAOA to solve NP-hard problems in average-case polynomial time.\ \ Moreover, they analogize the QAOA parameter optimization process to adiabatic evolution. The quantum adiabatic algorithm involves finding a high-energy eigenstate of some Hamiltonian by simulating a different time-dependent Hamiltonian on a quantum computer$^2$. This algorithm dates back to 2000 and was discovered by the same author; framing QAOA as a continuation (and in many ways, an improvement) of that work builds confidence that QAOA and other approximate optimization techniques may be the first sign of quantum supremacy in the “noisy intermediate-scale quantum” (NISQ) era.</p> <p>\section{QAOA for Max-Cut: Derivation} The max-cut problem for unweighted, undirected graphs can be stated as follows: color the nodes of graph $G$ with two colors such that the number of edges connecting nodes of different colors is maximized. For the graph shown below, the partition ${0, 1}, {2, 3}$ yields a cut value of 2, while the partition ${0, 3}, {1, 2}$ yields a cut value of 3 which is maximal. \begin{figure}[h] \centering \includegraphics[scale=0.5]{k4.PNG} \end{figure} The procedure of partitioning the node set of the graph into two disjoint sets can be numerically representing as assigning $s_i\in{-1, +1}$ for each node in the graph. Then, summing over all edges in the graph (and accounting for double counting the same edge), the cut value relating to any given partition of $G$ is [\frac{1}{2}\sum_{(u,v)\in G}{(1-s_us_v)}].</p> <p>\subsection*{Finding the Hamiltonian} To translate this problem into the language of the quantum computer, we need to envelop this information into a Hamiltonian matrix $C$, such that [C\ket{x}=C(x)\ket{x}] We first observe that the Pauli-Z matrix has eigenvalues $+1$ and $-1$ by inspection, with z-basis vectors as its eigenstates. Then, define $C:=\frac{1}{2}\sum_{(u,v)\in G}{(1-Z_uZ_v)}$ where $Z_k$ is a Pauli-Z gate operating on the k-th qubit of the input state. This matrix $C$ has the desired property. [C\ket{x}=C\ket{x_0x_1…x_n}\forall x_i\in {0,1}] [=\frac{1}{2}\sum_{(u,v)\in G}{(1-Z_uZ_v)}\ket{x_0x_1…x_n}] [=\frac{1}{2}\sum_{(u,v)\in G}\ket{x_0x_1…x_n}-\ket{x_0x_1…}(Z_u)\ket{x_u}\ket{x_{u+1}…x_{v-1}}(Z_v)\ket{x_v}\ket{…x_n}] [=\frac{1}{2}\sum_{(u,v)\in G}\ket{x_0x_1…x_n}-(-1)^{x_u}(-1)^{x_v}\ket{x_0x_1…x_n}] [=(\frac{1}{2}\sum_{(u,v)\in G}(1-(-1)^{x_u}(-1)^{x_v}))\ket{x_0x_1…x_n}=C(x)\ket{x}] Note that in the final step, any term of the summation yields 0 if and only if $x_u= x_v$ and 1 if and only if $x_u\neq x_v$. Since this Hamiltonian represents the cost function, it is referred to as the cost Hamiltonian. We also define a mixer Hamiltonian $M$, which will serve as an additional layer in the final quantum circuit that separates each sequence of cost Hamiltonian gates, as the sum of Pauli-X gates on each qubit: [M=\sum_{i}^{n}X_i] Note that for a graph with $|G|=n$ vertices, each of these matrices is $2^n\cross 2^n$ in dimensions. By describing them by summation, we avoid having to explicitly compute any elements of these matrices in the upcoming steps.</p> <p>\subsection*{Preparing the Ansatz} With these two Hamiltonians in hand, we can now proceed to the construction of the ansatz circuit for the Max-Cut QAOA problem. An “ansatz” is a general structure defined for a circuit. Some of the gates will be parameterized, and over the course of the optimization process we will find the best parameters for this ansatz to obtain near-optimal results. \ \ The QAOA setup requires us to apply alternating layers of unitary operators on the input state, which is an equal superposition. In other words, the state at the end of the circuit’s run should be [=e^{-i\beta_pM}e^{-i\alpha_pC}…e^{-i\beta_1M}e^{-i\alpha_1C}H^{\otimes n}\ket{0}] Here, the variable $p$ simply represents the number of repeated layers of cost and mixer gates that the ansatz contains. This is determined in the selection of the ansatz at the very start and not changed during the run. Increasing $p$ generally adds more power and expressivity to the circuit, at the cost of an increased runtime and a potentially prohibitive increase in cumulative error due to the increased number of fault-prone gates. \ \ Let us first examine the expressions of the form $e^{-i\alpha M}$. Substituting in our explicit definition for $M$, [e^{-i\alpha M}=e^{-i\alpha \sum_{j=1}^n X_j}] [=e^{-i\alpha X_1} e^{-i\alpha X_2} … e^{-i\alpha X_n}] [=\Pi_{j=1}^n e^{-i\alpha X_j}] Then, this part of the ansatz corresponds to a series of qubit-by-qubit rotations of angle $2\alpha$ in the x-basis. A more complicated situation arose with operators of the form $e^{-i\beta C}$. Note first that for the purposes of maximization, the coefficients on the summation and constants inside are irrelevant. Then, we can express the objective $\frac{1}{2}\sum_{(u, v)\in G}1-Z_uZ_v$ more succinctly as $C=\sum_{(u, v)\in G} -Z_uZ_v$. It should be evident that maximizing the action of the first Hamiltonian on the equal superposition state maximizes the second as well. Then, we can explore how the operator $e^{-i\beta C}$ acts on the four two-qubit states $x_u$ and $x_v$ could possibly be in. [e^{-i\beta C}\ket{00} = e^{i\beta Z_u\otimes Z_v}\ket{00} = e^{i\beta}\ket{00}] [e^{-i\beta C}\ket{01} = e^{i\beta Z_u\otimes Z_v}\ket{01} = e^{-i\beta}\ket{01}] [e^{-i\beta C}\ket{10} = e^{i\beta Z_u\otimes Z_v}\ket{10} = e^{-i\beta}\ket{10}] [e^{-i\beta C}\ket{11} = e^{i\beta Z_u\otimes Z_v}\ket{11} = e^{i\beta}\ket{11}] Then, the action of operator $e^{-i\beta C}$ on state $\ket{x_ux_v}$ for $x_u, x_v\in {0,1}$ is [e^{i(-1)^{x_u\oplus x_v} \beta}\ket{x_ux_v}] The state remains the same albeit with a phase shift dependent on the two qubit values; if $x_u=x_v$, the phase shift is $e^{i\beta}$, else it is $e^{-i\beta}$. The sequence of gates that achieves this result is $CNOT(u\to v)\otimes R_z(2\beta) \otimes CNOT(u\to v)$, where the $R_z$ gate represents a rotation by the specified angle in the z-basis.</p> <p>\section{QAOA for Max-Cut: Implementation} My full code is available here:\ https://colab.research.google.com/drive/1Bn_XCxoIdZVLRiPU_MsHozQikNMNS_4k?usp=sharing <br/> Outputs have been saved for easy viewing but the notebook is no longer linked to my IBM account. \ \<br/> The method \textsc{get_maxcut_ansatz} returns a QuantumCircuit parameterized by the values passed into it. The value of $p$ is inferred from the number of angle parameters provided. <br/> \begin{figure}[h] \centering \includegraphics[scale=0.5]{get-maxcut-ansatz.PNG} \end{figure}</p> <p>In the first step, a Hadamard transform is applied to the input qubits in the $\ket{00…0}$ state, placing them into an equal superposition. Next, $p$ layers of the alternating CNOT-RZ-CNOT and RX gates are added. Finally, all qubits are measured and the observed values are stored in classical registers.\ \ For a graph with 5 vertices and $p=1$, the ansatz looks like so.</p> <p>\begin{figure}[h] \centering \includegraphics[scale=0.5]{p1circuit.PNG} \end{figure}</p> <p>Note the cascade of CNOT-RZ-CNOT units, followed by a string of RX rotations on each wire. At the end, all qubits are measured. In contrast, consider this circuit for $p=2$.</p> <p>\begin{figure}[h] \centering \includegraphics[scale=0.5]{p2ansatz.PNG} \end{figure}</p> <p>As expected, it has two layers of alternating cost and mixer gates. Note that the parameter values for the RZ gates differ across layers. The same applies for the RX gates. \ <br/> The iterative workflow of the program was to first make measurements using the default trial parameter values assigned to the ansatz. Each set of $n$ measurements was interpreted as a partition of the node set into two groups, and the cut value of this partition was calculated. Over a large number of shots, this process was repeated until all shots were exhausted, when the cost function returns the “expected cut value” produced by running the quantum circuit to find an adequate partition. This was all packaged into a function that represented a mapping of $2p$ parameters to a scalar value (expected cut value) and fed into the Scipy \textsc{minimize} utility.</p> <p>\begin{figure}[h] \centering \includegraphics[scale=0.6]{optimize-maxcut.PNG} \end{figure}</p> <p>The \textsc{optimize_maxcut} function is responsible for encapsulating this optimization routine. It uses the COBYLA algorithm, the default suggested by Scipy. Note that this method returns three pieces of information: the best partition found, the cut value of that partition, and the full frequency distribution of each outcome for further analysis and visualization.\ \ Methods to create random graphs and calculate maximum cuts by brute force (iterating over all possible partitions) are also included for testing and debugging purposes.</p> <p>\subsection*{Experimental Results} Two example graphs are experimented with at the end of the Jupyter notebook. \begin{figure}[h] \centering \begin{subfigure} \centering \includegraphics[width=.4\linewidth, scale=0.5]{g1.png} G1 \end{subfigure} \begin{subfigure} \centering \includegraphics[width=.4\linewidth, scale=0.5]{g2.png} G2 \end{subfigure} \end{figure} \Max-cut QAOA is run for two settings of $p$ ($p=1$ and $p=2$) and on the QASM simulator or a real IBM quantum computer. Thus, there are eight total computations present in the notebook. In the table below, approximation ratios for all eight trials are presented (note that if the ratio is 1, then the optimal cut was found). \begin{figure}[h] \centering \includegraphics[scale=0.7]{approximation_ratios.PNG} \end{figure} \As a sanity check, we apply the \textsc{draw_maxcut_result} function on these graphs to show their partitions. The top four correspond to G1 and the bottom four to G2. All partitions match their calculated cut values. \pagebreak</p> <p>\begin{figure}[h] \centering \includegraphics[scale=0.6]{maxcut-viz.PNG} \caption*{Visualizations of graph partitions for G1 and G2} \end{figure}</p> <p>\section{Conclusions and Connections} One trend I noticed was that QASM simulations performed definitively better than real quantum computers in approximating the max-cut solution on either graph. Holding the $p$ value constant and comparing results for the same graph shows that running the circuit on IBM’s quantum computers always does worse than the simulated answer. This is unsurprising. The QASM simulator is by definition error-free, since it does not truly manipulate qubits to achieve its computations. Rather, it simulates quantum behavior in a deterministic manner, with classical registers describing the state of the $n$-qubit system exactly. On the other hand, IBM’s remote machine are prone to noise and have imperfect error mitigation, resulting in more overall noise in the data. This in turn reduces the prevalence of the correct partition occurring in the data. \ \ Looking closely at the QASM result, it is clear that increasing the value of $p$ yields better results. This is again unsurprising, because the ansatz with more layers has more degrees of freedom to model the desired behavior. With QASM, there are no drawbacks to using one or two more layers, barring a slight increase in overall run-time; however, when running the circuits on IBM’s quantum computers, increased depth is correlated with increased error, which is unacceptable given the relatively poor performance of the real machines on even $p=1$ architectures. \ <br/> Moreover, I think results would be improved if a better algorithm was chosen for the optimization routine. COBYLA is a general purpose, conventional optimization algorithm that is unaware of the innate randomness in quantum simulation results. When a measurement is made at the end of one of the circuit’s runs during the optimization routine, there is a chance that the wrong state is measured, especially with the introduction of small phase errors in real quantum computers. These small errors may manifest themselves in the ultimate measurement, and COBYLA will take these results at face value. If we were able to incorporate some quantum understanding into COBYLA, providing it with the udnerstanding that not all measurements are 100\% accurate, it may help the optimization converge faster and more accurately than it currently does.\ \ Given more time, I might wish to implement some of the more recent research into QAOA that has occurred in the last decade to improve my results. In a paper by Shaydulin et al (2021)$^4$, the authors present a methodology for exploiting the inherent symmetry of some problems in order to acheive faster run-times. Since the main bottleneck in the QAOA variational cycle is on the quantum side rather than the classical side, this would be a major improvement and would allow me to experiment with larger, denser graphs. In another paper by Marwaha (2021)$^3$, it is suggested that there may not be the trivial linear increase in modeling capability that I presented as fact earlier in the paper when we increase $p$. Indeed, it appears that for some graphs, the max-cut ansatz works better with fewer layers. This would be another matter of great interest to investigate further.</p> <p>\section*{References} \begin{enumerate} \item Farhi et al, “A Quantum Approximate Optimization Algorithm”. arXiv:1411.4028 \item Farhi et al, “Quantum Computation by Adiabatic Evolution”. arXiv:quant-ph/0001106 \item Marwaha, “Local classical MAX-CUT algorithm outperforms p=2 QAOA on high-girth regular graphs”. arXiv:2101.05513 \item Shaydulin et al, “Exploiting Symmetry Reduces the Cost of Training QAOA”. arXiv:2101.10296 \end{enumerate}</p>]]></content><author><name></name></author><summary type="html"><![CDATA[\section*{Abstract} Quantum approximate optimization algorithms (QAOA) are a class of quantum algorithms applicable to combinatorial optimization problems. In this paper, I present a general overview of the process of adapting a problem to a QAOA-based approach. Then, I apply QAOA to the Max-Cut Problem for unweighted graphs, showing the process of developing the appropriate circuit in Qiskit and assessing the validity of my measurements. }]]></summary></entry></feed>